# 神经网络原理

## 学习目标

- 目标
  - 说明神经网络的分类原理
  - 说明softmax回归
  - 说明交叉熵损失
- 应用
  - 无

神经网络的主要用途在于分类，那么整个神经网络分类的原理是怎么样的？我们还是围绕着损失、优化这两块去说。神经网络输出结果如何分类？

![神经网络如何分类](/images/神经网络如何分类.png)

 **神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。**

任意事件发生的概率都在0和1之间，且总有某一个事件发生（概率的和为1）。如果将分类问题中“一个样例属于某一个类别”看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。如何将神经网络前向传播得到的结果也变成概率分布呢？Softmax回归就是一个非常常用的方法。

## 1、softmax回归

Softmax回归将神经网络输出转换成概率结果

![softmax公式](/images/softmax公式.png)

![softmax回归](/images/softmax回归.png)

![softmax展开](/images/softmax展开.png)

* softmax特点

如何理解这个公式的作用呢？看一下计算案例

```python
假设输出结果为：2.3, 4.1, 5.6
softmax的计算输出结果为：
y1_p = e^2.3/(e^2.3+e^4.1+e^5.6)
y1_p = e^4.1/(e^2.3+e^4.1+e^5.6)
y1_p = e^5.6/(e^2.3+e^4.1+e^5.6)
```

**这样就把神经网络的输出也变成了一个概率输出**

![识别输出概率](/images/识别输出概率.png)

> 类似于逻辑回归当中的sigmoid函数，sigmoid输出的是某个类别的概率

#### 想一想线性回归的损失函数以及逻辑回归的损失函数，那么如何去衡量神经网络预测的概率分布和真实答案的概率分布之间的距离？

## 2、交叉熵损失 

### 2.1 公式

![交叉熵损失公式](/images/交叉熵损失公式.png)

为了能够衡量距离，目标值需要进行one-hot编码，能与概率值一一对应，如下图

![交叉损失理解](/images/交叉损失理解.png)

它的损失如何计算？

```python
0log(0.10)+0log(0.05)+0log(0.15)+0log(0.10)+0log(0.05)+0log(0.20)+1log(0.10)+0log(0.05)+0log(0.10)+0log(0.10)
```

**上述的结果为1log(0.10)，那么为了减少这一个样本的损失。神经网络应该怎么做？**所以会提高对应目标值为1的位置输出概率大小，由于softmax公式影响，其它的概率必定会减少。只要这样进行调整这样是不是就预测成功了！！！！！

### 提高对应目标值为1的位置输出概率大小

### 2.2 损失大小

神经网络最后的损失为平均每个样本的损失大小。对所有样本的损失求和取其平均值

#### 有了这两个关键部分，神经网络的分类就是这样去做的，但是它是如何优化这些输出概率的呢？

## 3、BP算法(了解)(Backpropagation)

神经网络当中充满大量的权重、偏置参数，这些参数都需要去进行优化。**之前我们接触的线性回归、逻辑回归通过梯度下降优化参数。这里也是一样，只不过由于神经网络的隐层可以增加很多层，那么这个过程需要一种规则。**

* **定义：梯度下降+链式求导规则**

### 3.1 BP算法过程

**1、前向传输（Feed-Forward）**

从输入层=>隐藏层=>输出层，一层一层的计算所有神经元输出值的过程。

**2、逆向反馈（Back Propagation）**

因为输出层的值与真实的值会存在误差，我们可以用均方误差来衡量预测值和真实值之间的误差。

* 在手工设定了神经网络的层数，每层的神经元的个数，学习率 η（下面会提到）后，BP 算法会先随机初始化每条连接线权重和偏置
* **对于训练集中的每个输入 x 和输出 y，BP 算法都会先执行前向传输得到预测值**
* **根据真实值与预测值之间的误差执行逆向反馈更新神经网络中每条连接线的权重和每层的偏好。**

### 3.2 总结

我们不会详细地讨论可以如何使用反向传播和梯度下降等算法训练参数。**训练过程中的计算机会尝试一点点增大或减小每个参数，看其能如何减少相比于训练数据集的误差，以望能找到最优的权重、偏置参数组合**。

![传播算法演示](/images/传播算法演示.png)

![网络优化动态图](/images/网络优化动态图.gif)

## 4、softmax、交叉熵损失API

- tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None,name=None)
  - 计算logits和labels之间的交叉损失熵
  - labels:标签值（真实值）
  - logits：样本加权之后的值
  - return:返回损失值列表
- tf.reduce_mean(input_tensor)
  - 计算张量的尺寸的元素平均值