# 线性神经网络局限性

## 学习目标

- 目标
  - 了解网络激活函数的作用
  - 知道网络加深的线性关系
- 应用
  - 无

任意多个隐层的神经网络和单层的神经网络都没有区别，**而且都是线性的，而且线性模型的能够解决的问题也是有限的** 

## 1、 更复杂抽象的数据

一个单隐含层有更多的神经元，你就能捕捉更多的特征。而且有更多隐层，意味着你能从数据集中提取更多复杂的结构。

![螺旋结构](/images/螺旋结构.png)

### 1.1 增加网络深度

![线性演示效果](/images/线性演示效果.png)

### 1.2 使用非线性激活函数

![relu增加深度](/images/relu增加深度.png)



## 2、神经网络更多特性

### 2.1黑盒子特点

* 增加网络的深度的确能够达到效果，但是增加多少？这是一个不确定的问题，或者可以改变神经元的一些特点，改变结构同时增加网络深度，这些都有很多结构进行了尝试。
* 不清楚网络内部每个神经元到底在什么事情

### 2.2 更多发展 

更多神经元 + 更深的网络 = 更复杂的抽象。这也是简单的神经元如何变得更聪明，并在图像识别、围棋这些特定问题上表现如此之好的原因。

#### 2.2.1 神经网络拓展介绍

> - 神经网络的种类
>   - 基础神经网络：线性神经网络，BP神经网络，Hopfield神经网络等﻿
>   - 进阶神经网络：玻尔兹曼机，受限玻尔兹曼机，递归神经网络等﻿
>   - 深度神经网络：深度置信网络，卷积神经网络，循环神经网络，LSTM网络等

![inception结构](/images/inception结构.png)

**Inception：谷歌公开的一个图像识别模型 **

### 2.3 两大挑战:计算能力和训练数据

在此文章中，我们看到了一些 TensorFolw Playground 演示解释了神经网络的机制和能力。就像你看到的那样，这一技术的基础非常简单。每一个神经元只将一个数据点分类成两个类别中的一个。然而，通过有更多的神经元和深度层，一个神经网络能提取出训练数据集中隐藏的见解和复杂模式，并且建立抽象的层级结构。

接下来的问题是，为什么如今还不是每个人都在使用这一伟大的技术？这是因为神经网络还有两大挑战。

* 第一个是训练深度神经网络需要大量的算力。
* 第二，它们需要大量的训练数据集。一个强力的 GPU 服务器可能要花费数天、甚至数周的时间，才能使用数百万张图像的数据集训练出一个深度网络。

而且，为了得到最好的训练结果，需要结合不同的网络设计与算法，并进行大量的试错。如今，一些研究者会使用几十个 GPU 服务器，甚至超级计算机来进行大规模分布式的训练。但在不久的将来，全面管理的分布式训练与预测服务——比如[谷歌 TensorFlow 云机器学习平台](https://cloud.google.com/ml/)——可能会解决这些问题，为大家提供成本合理的基于云的 CPU 和 GPU，而且也可能会把大型的或深度的神经网络的能力开放给每一个人。

